\input{config.tex}
\begin{document} 

\begin{multicols}{5}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\section*{Teorija aproksimacije}
\begin{align*}
    X\quad \dots& \quad \text{vekt. prostor katerega el. aproksimiramo} \\
    S \subseteq X \quad \dots& \quad  \text{podprostor aproksimantov} \\
    A: X \to S \quad \dots& \quad \text{operacijska shema (operator)}
\end{align*}

Prileganje aproksimanta ocenimo z normo:
\begin{itemize}
    \item \textbf{Neskončna norma} 
    \[ \| f \|_{\infty, [a,b]} = \max_{x \in [a, b]} | f(x) | \]
    \textit{Numerični približek:} na intervalu $[a, b]$ izberemo konkčno mnogo točk $a \leq x_0 < x_1 < \dots < x:n \leq b$. 
    \[ \| f \|_{\infty, [a,b]} = \max_{i = 0, \dots, n} | f(x_i) | \]
    \item \textbf{Druga norma} 
    \begin{align*}
        \| f \|_{2} &= \sqrt{\langle f, f \rangle} &
        \langle f, g \rangle &= \int_a^b f(x) g(x) \rho(x) dx 
    \end{align*}
    Standardni skalarni produkt: $\rho \equiv 1$.

    \textit{Numerični približek:} vzamemo diskretni skalarni produkt.
    Na intervalu $[a, b]$ izberemo konkčno mnogo točk $a \leq x_0 < x_1 < \dots < x:n \leq b$.
    \[\langle f, g \rangle = \sum_{i=0}^n f(x_i) g(x_i) \rho(x_i) dx\]
\end{itemize}

\subsection*{Optimalni aproksimacijski problem}
Za $f \in X$ iščemo aproksimant $\hat{f} \in S$, da je
\[ \| f - \hat{f} \| = \min_{s \in S} \| f - s \| \]

\subsection*{Aproksimacija po metodi najmanjših kvadratov}
Naj bo $X$ vektorski prostor nad $\mathbb{R}$ s skalarnim produktom $\langle \cdot, \cdot \rangle$ in normo $\| \cdot \|_2 = \sqrt{\langle \cdot, \cdot \rangle}$ 
\[ S = \text{Lin}\{l_1, l_2, \dots, l_n\} \subseteq X \]

Iščemo \textbf{element najboljše aproksimacije po MNK} $f^* \in S$, da $\| f - f^* \| = \min_{s \in S} \| f - s \| $

\textit{Izrek:} $f^*$ je el. najboljše aproksimacije po MNK $\iff$ $f-f^* \perp S$ $\iff$
$f-f^* \perp l_i \quad \forall i = 1,\dots n$

\[ f^* = \alpha_1 l_1 + \dots + \alpha_n l_n\]

Iz zgornjega izreka sledi:
\begin{align*}
\langle f - f^*, l_i \rangle &= 0 \quad \forall i\\
\langle f - \sum_{j=1}^n \alpha_j l_j, l_i \rangle &= 0 \quad \forall i\\
\langle f, l_i\rangle - \sum_{j=1}^n \alpha_j \langle  l_j, l_i \rangle &= 0 \quad \forall i\\
\end{align*}
V matrični obliki:
\[
    \underbrace{\begin{bmatrix}
        \langle l_1, l_1 \rangle & \dots & \langle l_n, l_1 \rangle \\
        \vdots & \ddots & \vdots \\
        \langle l_1, l_n \rangle & \dots & \langle l_n, l_n \rangle
    \end{bmatrix}}_{\text{Grammova matrika $G$}}
    \begin{bmatrix}
        \alpha_1 \\
        \vdots \\
        \alpha_n
    \end{bmatrix}
    =
    \begin{bmatrix}
        \langle f, l_1 \rangle \\
        \vdots \\
        \langle f, l_n \rangle
    \end{bmatrix}
\]

$G$ je simetrična pozitivno definitna matrika. Numerično tak sistem rešimo z razcepom Choleskega.


Reševanje sistema linearnih enačb se izognemo tako, da bazo za $S$ ortonormiramo. Tedaj je $G = I$ in
\[ f^* = \sum_{i=1}^n \langle f, l_i \rangle l_i \]

\subsubsection*{Gram-Schmidtova ortogonalizacija}
Definirajmo projekcijo vektorja $v$ na $u$
\[\textmd{proj}_u(v) = \frac{\langle v,u \rangle}{\langle u,u \rangle}u\]
Če želimo \emph{orotogonalizirati} $k$ linearno neodvisnih vektorjev $v_1, ..., v_k$, uporabimo postopek:
\begin{equation*}
    \begin{aligned}
    u_1 &= v_1 \\
    u_2 &= v_2 - \textmd{proj}_{u_1}(v_2) \\
    u_3 &= v_3 - \textmd{proj}_{u_1}(v_3) - \textmd{proj}_{u_2}(v_3) \\
    &\; \ \vdots \\
    u_k &= v_k - \sum_{j=1}^{k-1} \textmd{proj}_{u_j}(v_k)
    \end{aligned}
\end{equation*}


\subsection*{Enakomerna aproksimacija zveznih funkcij s polinomi}
Za dano funkcijo $f \in \mathcal{C}([a,b])$ iščemo \textbf{polinom najboljše enakomerne aproksimacije} $p^* \in \mathbb{P}_n$, za katerega velja
\[ \| f - p^* \|_{\infty, [a,b]} = \min_{p \in \mathbb{P}_n} \| f - p\|_{\infty, [a,b]} = \min_{p \in \mathbb{P}_n} \max_{x \in [a, b]} | f(x) - p(x) | \]

\textit{Izrek:} Naj bo $f \in \mathcal{C}([a, b])$. Če je polinom $p \in \mathbb{P}_n$ takšen,
da \textbf{residual} $r = f - p$ doseže svojo normo $\| r \|_{\infty, [a,b]}$ alternirajoče:
$n+2$ tokčah $x_i$, $a \leq x_0 < \dots < x_{n+1} \leq b$ 
\[ r(x_i)r(x_{i+1}) < 0 \qquad \forall i=0, \dots, n \]
potem je $p$ polinom najboljše enak. aproks. za $f$ na $[a, b]$.

\subsubsection*{Remesov postopek}
Vhodni podatki: funkcija $f$, interval $[a,b]$, stopnja $n$, toleranca $\varepsilon$

Izberi množico točk $E_0 = \{ x_i,\ a \leq x_0 < \dots < x_{n+1} \leq b \}$.

\textbf{Minimaks} za funkcijo $f$ na $E$ je
\[M_n(f, E) = \min_{p \in \mathbb{P}} \max_{x \in E} | f(x) - p(x) | = |m|\]

Ponavljaj $k = 0, 1, 2, \dots$:
\begin{itemize}
    \item Poišči polinom $p_k^* \in \mathbb{P}_n$, ki zadošča pogoju:
    \[ f(x_i) - p_k^* = (-1)^i m \qquad \forall i = 0, 1, \dots, n+2 \]
    \item Poišči ekstrem residuala $r_k = f - p_k^*$. To je $u \in [a,b]$, da
    \[ |r_k(u)| = \| r_k \|_{\infty, [a,b]} \]
    \item Če je $|r_k(u)| - |m| < \epsilon$, potem končaj in vrni $p^* = p_k^*$. 
    Sicer pa naredimo zamenjavo točk v množici $E_k$ z $u$ tako, da ogranimo alternacijo residuala.
\end{itemize}

\subsection*{Interpolacija}
Podane so vrednosti izbarne funkcije $f$ v $n+1$ paroma različnih točkah $x_i$ (\textbf{interpolacijske točke}),
iščemo neko preprostejšo funkcijo $g$ (\textbf{interpolacijska funkcija}), ki zadošča pogoju:
\[ f(x_i) = g(x_i) \qquad i = 0, 1, \dots, n \]

\subsection*{Polinomska interpolacija}
$f \in \mathcal{C}([a,b])$, $a \leq x_0 < \dots < x_n \leq b$. Iščemo polinom
\[ p(x) = a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n \]
ki zadošča pogoju $f(x_i) = p(x_i)$ za $i = 0, 1, \dots, n$. 
\[
\underbrace{\begin{bmatrix}
    1 & x_0 & x_0^2 & \dots & x_0^n \\
    1 & x_1 & x_1^2 & \dots & x_1^n \\
    \vdots & &        &       & \vdots \\
    1 & x_n & x_n^2 & \dots & x_n^n \\
\end{bmatrix}}_{\text{Vandermondova matrika $V$}}
\begin{bmatrix}
    a_0 \\
    a_1 \\
    \vdots \\
    a_n
\end{bmatrix}
=
\begin{bmatrix}
    f(x_0) \\
    f(x_1) \\
    \vdots \\
    f(x_n) \\
\end{bmatrix}
\]
\[ det V = \prod_{0 \leq j < i \leq n} (x_i - x_j) \neq 0\]
Vidimo, da je polinom $p$ enolično določen.

\subsubsection*{Lagrangeova oblika interpolacijskega polinoma}
\[ l_{i, n}(x) = \prod_{\substack{j=0 \\ j\neq i}}^n \frac{x-x_j}{x_i - x_j} \qquad i = 0, \dots, n \]

% ni ključno
Velja:
\[ 
l_{i,n}(x_j) = 
\begin{cases}
    1 & i = j \\
    0 & \text{sicer}
\end{cases}
\]
\textit{Lema:} $l_{i, n}$ so baza za $\mathbb{P}_n$.
%

Interpolacijski polinom v lagrangeovi obliki:
\[ p(x) = \sum_{i=0}^n f(x_i) l_{i,n}(x)\]

\subsubsection*{Newtonova oblika zapisa interpolacijskega polinoma}

Za bazo izberemo prestavljene potence: $1$, $(x-x_0)$, $(x-x_0)(x-x_1)$, \dots  


\textbf{Deljena diferenca} $[x_0, x_1, \dots, x_k] f$ je vodilni koeficient interpolacijskega polinoma
stopnje $k$, ki se s funkcijo $f$ ujema v točkah $x_0, x_1, \dots, x_k$. Sledi:
\[ p_k(x) = p_{k-1}(x) + [x_0, x_1, \dots, x_k] f (x-x_0)\dots (x-x_{k-1})\]
Rekurzivna zveza:
\begin{multline*}
    [x_i, x_{i+1}, \dots, x_{i+k}]f = \\
    = \begin{cases}
        \frac{f^{(k)}(x_i)}{k!} & {\scriptstyle x_i = x_{i+1} = \dots = x_{i+k}} \\
        \frac{[x_{i+1}, \dots, x_{i+k}]f- [x_i, \dots, x_{i+k-1}]f}{x_k-x_0} & {\scriptstyle x_i \neq x_{i+k} }
    \end{cases} 
\end{multline*} 

Newtonova oblika zapisa je torej:
\[ p(x) = \sum_{i=0}^n [x_0, \dots, x_i]f (x-x_0)\dots (x-x_{i-1}) \]

\textit{Če želimo, da se polinom n neki toči ujema tudi v $k$-tem odvodu, to točko $k$-krat ponovimo.}

\subsubsection*{Računanje vrednosti interpolacijskega polinoma v Newtonovi obliki}
\[ p(x) = d_0 1 + d_1(x-x_0) + d_2(x-x_0)(x-x_1) + \dots \]

\begin{koda}
vhodni podatki: $x_0, \dots, x_n, \ d_0, \dots, d_n, \ x$
$v_n \leftarrow d_n$
za $i = n-1, \dots, 0$:
    $v_i \leftarrow d_i + (x-x_i) v_{i+1}$

vrni $v_0$
\end{koda}

\subsubsection*{Kako izbrati točke na $[a,b]$?}
\begin{itemize}
    \item \textbf{Ekvidistantne točke}: $h = \frac{b-a}{n}$, $x_i = a + ih$
    \item \textbf{Izbira pri kateri je dosežen minimum}
    \item \textbf{Čibiševe točke}
    \[ x_i = \frac{a+b}{2} + \frac{a-b}{2}\cos\left(\frac{2i+1}{2n+2} \pi\right) \qquad i = 0, \dots, n\]
\end{itemize}


\section*{Numerično odvajanje}
Kot približek odvoda funkcije $f$ v izbrenem $x$ vzamemo verdnost dovoda interpolacijskega
polinoma na točkah $x_0, \dots, x_n$ pri tem $x$.

\begin{align*}
    f(x) = p(x) + \omega(x)[x_o, x_n, x]f && \omega(x) = \prod_{i=0}^n (x-x_i) \\
    p(x) = \sum_{i=0}^n f(x_i) l_{i,n} && l_{i,n}(x) = \prod_{\substack{j=0 \\ j \neq i}}^n \frac{x-x_j}{x_i - x_j}
\end{align*}
Odvajamo 
\[
f'(x) = 
\underbrace{p'(x)}_{\mathclap{\text{približek}}} + 
\underbrace{\omega'(x)[x_0, \dots, x_n, x] f + \omega(x)([x_o, \dots, x_n, x] f)'}_{\text{napaka}}
\]

Odvod deljene niference:
\[ \frac{d}{dx} [x_0, \dots, x_n, x] f = [x_0, \dots, x_n, x, x] f\]

Interpolacijske točke ponavadi izberemo ekvidistntno, za $x$ pa izberemo kar eno izmed
interpolacijskih točk (npr. $x_k$). Potem dobimo:
\[ 
    f'(x_k) = p'(x_k) + 
    \underbrace{\omega'(x_k) \frac{f^{(n+1)}(\xi)}{(n+1)!}}_{\text{napaka}}
\]

\section*{Numerično integriranje}

Namesto $f$ integriramo aproksimacijski polinom na izbranih točkah $a \leq x_0 < \dots < x_n \leq x_n$ (\textbf{vozli}).
\[
f(x) = p(x) + \omega(x) [x_0, \dots, x_n, x] f
\]
\begin{align*}
    p(x) = \sum_{i=0}^n f(x_i) l_{i,n}(x) && \omega(x) = \prod_{i=0}^n (x-x_i)
\end{align*}

Kvadraturna formula oz. integracijsko pravilo
\[
\underbrace{\int_a^b f(x) dx}_{Sf} =
\underbrace{\int_a^b p(x) dx}_{Ff} +
\underbrace{\int_a^b \omega(x)[x_0, \dots, x_n, x]f dx}_{Rf} 
\]

\begin{multline*}  
    \int_a^b p(x) dx = 
    \int_a^b \sum_{i=0}^n f(x_i) l_{i,n}(x) dx = \\
    = \sum_{i=0}^n f(x_i) \underbrace{\int_a^b l_{i,n}(x) dx}_{\text{utež } A_i} = 
    \sum_{i=0}^n A_i f(x_i)
\end{multline*}

\subsubsection*{Red integracijskega pravila}
je enak $m$, če je pravilo točno ($Rf = 0$) za vse polinome stopnje manjše ali enake $m$ ni pa točno ($Rf \neq 0$) za polinome višje stopnje kot $m$.

\textit{Dovolj je preveriti bazne polinome.}

\subsection*{Newton-Cotesova pravila}
Vozle izberemo ekvidistantno.
\begin{align*}
    h = \frac{b-a}{n} && x_0 = a && x_i = x_0 + ih; \quad i = 0, 1, \dots, n
\end{align*}

Ločimo pravila \textbf{zaprtega tipa} pri katerih upoštevamo krajišča in \textbf{odprtega tipa} pri katerih jih ne.

\subsubsection*{Trapezno pravilo}
\[ \int_{x_0}^{x_1} f(x) dx = \frac{h}{2} (f(x_0) + f(x_1)) + Rf\]
\[ Rf = \int_{x_0}^{x_1} (x-x_0)(x-x_1) [x_0, x_1, x] f dx = -\frac{h^3}{12} f''(\xi)\]

\subsubsection*{Simpsonovo pravilo}
\[ \int_{x_0}^{x_2} f(x) dx = \frac{h}{3} (f(x_0) + 4f(x_1) + f(x_2)) + Rf\]
\[ Rf = -\frac{h^5}{90} f^{(4)}(\xi)\]

\subsubsection*{Ocena napake in Richardsonova ekstrapolacija}
Integral računamo s sestavljenim Newton-Cotesovim pramilom. S $F_h$ označimo približek, ki ga dobimo s 
korakom $h$. Predpostavimo, da velja:

\[ I = F_h + c_0 h^p + \mathcal{O}\left( h^{p+1} \right)\]

Kjer je $c_0$ konstanta odvisna samo od $h$.

Natančnješi (ekstrapoliran) približek:
\[ I = \frac{2^p F_{h/2} - F_h}{2^p - 1} + \mathcal{O}\left(h^{p+1}\right)\]

Ocena za napako iz primerjave $F_h$ in $F_{h/2}$:
\begin{align*}
    I = F_{h/2} + \underbrace{\frac{F_{h/2} - F_h}{2^p - 1}}_{\mathclap{\text{ocena napake $F_{h/2}$}}} + \mathcal{O}\left( h^{p+1} \right) \\
    I = F_{h} + \underbrace{2^p\frac{F_{h/2} - F_h}{2^p - 1}}_{\mathclap{\text{ocena napake $F_{h}$}}} + \mathcal{O}\left( h^{p+1} \right) \\
\end{align*}
\subsection*{Gaussova pravila}
Vozle ($x_i$) in uteži ($A_i$) izračunamo tako, da bo pravilo čim višjega reda.

\[ \int_a^b f(x) dx = \sum_{i=0}^n A_i f(x_i) + Rf \]

Imamo $2(n+1)$ neznank, ki jih doličimo tako, da bo pravilo reda $2n+1$:
\[ \int_a^b x^k dx = \sum_{i=0}^n A_i x_i^k \qquad k=0,1,\dots, 2n+1\]

Želimo ločiti enačbe za neznanke uteži od neznank za vozle. Vozle laho izračunamo iz linearnega sistema ($n+1$) enačb:
\begin{align*}
    \omega(x) = (x-x_0)\dots (x-x_n) \\
    \int_a^b \omega(x) x^k dx = 0 \qquad k = 0,\dots, n
\end{align*}

\textit{Izrek:} za funkcijo $f \in \mathcal{C}^{2n+2}([a,b])$ je napaka Gaussovega pravila reda $2n+1$
\[ Rf = \frac{f^{(2n+2)}(\xi)}{(2n+2)!} \int_a^b \omega^2(x) dx \]

\subsection*{Integrali v več dimenzijah}
Fubinijev izrek:
\begin{multline*}
    I = \iint_{[a,b]\times [c,d]} f(x, y) dx dy = \\ 
    = \int_a^b dx \int_c^d f(x,y) dy = \int_c^d dy \int_a^b f(x, y) dx
\end{multline*}


\section*{Reševanje diferencialnih enačb}


% ni ključno
\textit{Eksistenčni izrek za začetni probelm:} $y' = f(x,y)$, $y(a) = y_a$. Naj bo $f$
na območju $D$ okrog $(a, y_a)$ zvezna funkcija in Lipschitzova v drugi spremenljivki:
\[ |f(x,y) - f(x, \tilde{y}) | \leq C |y-\tilde{y}| \]
Potem obstaja nek podinterval $[\alpha, \beta]$, ki vsebuje $a$, na katerem rešitev
začetnega probelma DE obstaja in je enolična.
%

\subsection*{Eksplicitna Eulerjeva metoda}
\[ y_{n+1} = y_n + h f(x_n, y_n) \qquad x_{n+1} = x_n + h\]

\subsection*{Implicitna Eulerjeva metoda}
\[ y_{n+1} = y_n + hf(x_{n+1}, y_{n+1}) \qquad x_{n+1} = x_n + h\]

Za izračun $y_{n+1}$ moramo rešiti nelinearno enačbo. Uporabio lahko navadno iteracijo:
\[ y_{n+1}^{(r)} = g(y_{n+1}^{(r-1)}) \qquad g(y_{n+1}) = y_n + hf(x_{n+1}, y_{n+1}) \]

Za začetni približek $y_{n+1}^{(0)}$ vzamemo kar $y_n$.

\subsection*{Trapezno pravilo}
\[ y_{n+1} = y_n + \frac{h}{2}\left(f(x_n, y_n) + f(x_{n+1}, y_{n+1})\right)\]

\subsection*{Globalna in lokalna napaka}
Globalna napaka v točki $x_n$:
\[ |y(x_n) - y_n | \]
Globalna napaka:
\[\max_{0 \leq n \leq m} |y(x_n) - y_n |\]

Metoda je \textbf{reda} $r$, če velja
\[ \max_{0 \leq n \leq m} |y(x_n) - y_n| = \text{konst.} h^r + \mathcal{O}\left(h^{r+1}\right) = \mathcal{O}\left(h^r\right)\]

Lokalna napaka v točki $x_n$ je razlika med točno rešitvijo v $x_n$ in njenim numeričnim približkom $y_n$
ob predpostavki, da se točna in numerična rešitev ujemata v vseh prejšnjih korakih:
\[ \tau_n(h) = y(x_n) - y_n \qquad y(x_k) = y_k; \ \forall k < n\]

Pri določanju lokalne napake si pomagamo z razvojem v Taylorjevo vrsto po $h$ okoli $x$.
\[ y(x+h) = y(x) + hy'(x)+\frac{h^2}{2!}y''(x) + \frac{h^3}{3!}y^{(3)}(x) + \dots\]
\begin{multline*}
    f(x + \Delta x, y + \Delta y) = f(x,y) + f_x(x,y)\Delta x + f_y(x,y) \Delta y + \\
    + \frac{1}{2!}\left( f_{xx}(x,y)\Delta x^2 + 2 f_{xy}(x,y) \Delta x \Delta y + f_{yy}(x,y) \Delta y^2\right) + \dots
\end{multline*}

Če je red lokalne napake $r$, je red metode $r-1$.

\subsection*{Runge Kutta metode}
Izračunamo $s$ koeficientov
\begin{gather*}
    k_i = hf(x_n + \alpha h, y_n + \sum_{j=1}^s \beta_{ij} k_j) \qquad i = 1, 2, \dots, s \\
    y_{n+1} = y_n + \sum_{i=1}^s \gamma k_i
\end{gather*}
Pri tem so $\alpha_i$, $\beta_{ij}$ in $\gamma_i$ koeficienti, ki so predstavljeni v \textbf{Butcherjevi shemi}:
\begin{center}
    \begin{tabular}[]{c | c c c}
        $\alpha_1$ & $\beta_{11}$ & $\dots$ & $\beta_{1s}$ \\
        $\alpha_2$ & $\beta_{21}$ & $\dots$ & $\beta_{2s}$ \\
        $\vdots$ &                &         & $\vdots$ \\
        $\alpha_s$ & $\beta_{s1}$ & $\dots$ & $\beta_{ss}$ \\ \hline
                 & $\gamma_{1}$ & $\dots$ & $\gamma_{s}$ \\

    \end{tabular}
\end{center}
Veljati mora:
\[\alpha_i = \sum_{j=1}^{s} \beta_{ij} \qquad \sum_{i=1}^s \gamma = 1\]

\begin{multicols*}{3}
    
    \textbf{Eulerjeva}
    \begin{center}
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}[]{c | c c}
            $0$ & 0 & 0 \\
            $\frac{1}{2}$ & $\frac{1}{2}$ & 0 \\ \hline
            & 0 & 1 \\
            
        \end{tabular}
    \end{center}
    red: 2

    \textbf{Heunova}
    \begin{center}
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}[]{c | c c}
            $0$ & 0 & 0 \\
            $1$ & $1$ & 0 \\ \hline
            & $\frac{1}{2}$ & $\frac{1}{2}$ \\
            
        \end{tabular}
    \end{center}
    red: 2
    
    \textbf{Diag. impl.}
    \begin{center}
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}[]{c | c c}
            $0$ & 0 & 0 \\
            $1$ & $\frac{1}{2}$ & $\frac{1}{2}$ \\ \hline
            & $\frac{1}{2}$ & $\frac{1}{2}$ \\
            
        \end{tabular}
    \end{center}
    red: 2
\end{multicols*}

\textbf{4 stopenjska R-K metoda}
\begin{center}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}[]{c | c c c c}
        $0$ & 0 &  &  &  \\
        $\frac{1}{2}$ & $\frac{1}{2}$  & 0 &  &  \\
        $\frac{1}{2}$ & 0  & $\frac{1}{2}$  & 0 &  \\
        $1$ & 0  & 0  & 1 & 0 \\ \hline
                 & $\frac{1}{6}$ & $\frac{2}{6}$ & $\frac{2}{6}$ & $\frac{1}{6}$ \\

    \end{tabular}
\end{center}
red: 4

\subsection*{Sistemi diferencialnih enačb 1. reda}
\begin{align*}
    y_1' &= f_1(x, y_1, \dots, y_d) & y_1(a) &= y_{1,a} \\
        &\vdots                     &        &\vdots \\
    y_d' &= f_d(x, y_d, \dots, y_d) & y_d(a) &= y_{d,a}
\end{align*}
Vektorski zapis:
\begin{align*}
    Y &= \begin{bmatrix}
        y_1 \\
        \vdots \\
        y_d
    \end{bmatrix}
    &\qquad
    Y:& [a,b] \to \mathbb{R}^d \\
    F &= \begin{bmatrix}
        f_1 \\
        \vdots \\
        f_d
    \end{bmatrix}
    &\qquad
    F:& [a,b] \times \mathbb{R}^d  \to \mathbb{R}^d 
\end{align*}

\[ Y' = F(x, Y) \qquad Y(a) = Y_a = \begin{bmatrix}
    y_{1,a} \\
    \vdots \\
    y_{d,a}
\end{bmatrix}\]
Vse prej naštete metode lahko direktno uporabimo za reševanje sistemov.

\subsection*{Diferencialne enačbe višjega reda}
\begin{gather*}    
    y^{(p)} = f(x, y, y', \dots, y^{(p-1)}) \\
    y(a) = y_{a,0} \qquad y'(a) = y_{a,1} \qquad y'(p-1) = y_{a,p-1} 
\end{gather*}   
Problem prevedemo na sistem diferencialnih enačb 1. reda.
Uvedemo nove neznane funkcije $z_1, z_2, \dots, z_{p-1}$:
\begin{align*}
    z_1 &= y' \\
    z_2 &= z_1' = y'' \\
    & \vdots \\
    z_{p-1} &= z_{p-2}' = y^{p-1} \\
    z_{p-1}' &= y^{(p)} = f(x, y, z_1, \dots, z_{p-1})
\end{align*}
\begin{align*}
    \begin{bmatrix}
        y \\
        z_1 \\
        \vdots \\
        z_{p-2} \\
        z_{p-1} \\
    \end{bmatrix}'
    &=
    \begin{bmatrix}
        z_1 \\
        z_2 \\
        \vdots \\
        z_{p-1} \\
        f(x,y, z_1, ...\, , z_{p-1}) \\
    \end{bmatrix}
    &
    \begin{bmatrix}
        y(a) \\
        z_1(a) \\
        \vdots \\
        z_{p-2}(a) \\
        z_{p-1}(a) \\
    \end{bmatrix}
    &=
    \begin{bmatrix}
        y_{a,0} \\
        a_{a,1} \\
        \vdots \\
        a_{a,p-2} \\
        a_{a,p-1} \\
    \end{bmatrix}
\end{align*}

\subsection*{Več čelenske metode}
Pri izračunu približkov $y_n = y(x_n)$ uporabimo $k$ vrednosti: $y_{n-1}, y_{n-2}, \dots, y_{n-k}$.

Splošna linearna $k$-členska metoda:
\[ y_n = \sum_{i=1}^k \alpha_i y_{n-i} + h \sum_{i=0}^k \beta_i \underbrace{f(x_{n-i}, y_{n-i})}_{f_{n-i}} \]

Pri tem so $@a_i$ in $\beta_i$ prosti koeficienti. Če je $\beta_0 = 0$, je metoda eksplicitna, sicer pa implicitna.

Prvih $k$ vrednosti $y_0, y_1, \dots, y_{k-1}$ moramo izračunti s kako od enočlenskih metod (njen red mora bit vsaj toliko, kot red $k$-členske metode).

\subsubsection*{Adamsove metode}
Enačbo $y' = f(x,y)$ integriramo na intervalu $[x_{n-1}, x_n]$, funkcijo $f(x, y(x))$ pa nadomestimo z interpolacijskim
polinomom na točkah $x_{n-k}, x_{n-k+1}, \dots, x_{n-1}$ (eksplicitna) ali  $x_{n-k}, x_{n-k+1}, \dots, x_{n}$ (implicitna).

Eksplicitne: $k = 2, 3, 4$ (po vrsti), red = $2, 3, 4$:
\begin{align*}
    y_n &= y_{n-1} + h\left( \frac{3}{2} f_{n-1} - \frac{1}{2} f_{n-2}\right) \\
    y_n &= y_{n-1} + h\left( \frac{23}{12} f_{n-1} - \frac{4}{3} f_{n-2} + \frac{5}{12}f_{n-3}\right) \\
    y_n &= y_{n-1} + h\left( \frac{55}{24} f_{n-1} - \frac{59}{24} f_{n-2} + \frac{37}{24}f_{n-3} - \frac{9}{24} f_{n-4}\right) \\
\end{align*}

Implicitne: $k = 2, 3, 4$ (po vrsti), red = $3, 4, 5$:
\begin{align*}
    y_n &= y_{n-1} + h\left( \frac{5}{12} f_{n} + \frac{8}{12} f_{n-1} - \frac{1}{12} f_{n-2}\right) \\
    y_n &= y_{n-1} + h\left( \frac{9}{24} f_{n} + \frac{19}{24} f_{n-1} - \frac{5}{24} f_{n-2} + \frac{1}{24}f_{n-3}\right) \\
    y_n &= y_{n-1} + {\scriptstyle h\left( \frac{251}{720} f_{n} + \frac{646}{720} f_{n-1} - \frac{264}{720} f_{n-2} + \frac{106}{720}f_{n-3} - \frac{19}{720} f_{n-4}\right) }
\end{align*}

\subsubsection*{Milneove metode}
Ideja za izpeljavo: DE $y' = f(x,y)$ integriramo na $[x_{n-k}, x_n]$, integral pa aproksimiramo z Newton-Cotesovimi pravili.

Odprta pravila $\rightarrow$ eksplicitne metode \\
Zaprta pravila $\rightarrow$ implicitne metode 

Primer eksplicitne metode: $k = 3$, red = 4:
\[ y_n = y_{n-4} + \frac{4h}{3} (2f_{n-1} - f_{n-2} + 2f_{n-3}) \]

Primer implicitne metode: $k = 2$, red = 4:
\[ y_n = y_{n-2} + \frac{h}{3} (f_{n} + 4f_{n-1} + f_{n-2}) \]

\subsubsection*{BDF metode}
To so implicitne metode. Odvod v enačbi $y'(x_n) = f(x_n, y(x_n))$ aproksimiramo z diferencami.

Primeri (red je enak $k$):
\begin{align*}
    y_n &= \frac{4}{3} y_{n-1} - \frac{1}{3} y_{n-2} + \frac{2}{3}h f_n \\
    y_n &= \frac{18}{11} y_{n-1} - \frac{9}{11} y_{n-2} + \frac{2}{11} y_{n-3} + \frac{6}{11} h f_n \\
    y_n &= \frac{48}{11} y_{n-1} - \frac{36}{25} y_{n-2} + \frac{16}{25} y_{n-3} -q  \frac{3}{25} y_{n-3} + \frac{12}{25} h f_n \\
\end{align*}

\section*{Numerično računanje lastnih vrednosti}
\begin{align*}
    A \in \mathbb{C}^{n\times n} && x \in \mathbb{C}^n && \lambda \in \mathbb{C}
\end{align*}
$Ax = \lambda x$ $\implies$ $x$ je desni lastni vektor $\lambda$ pa pripadajoča lastna vrednost\\
$y^HA = \lambda y^H = \overline{\lambda} y$ $\implies$ $y$  je levi lastni vektor $\lambda$ pa pripadajoča lastna vrednost

Matriko $A$ se da \textbf{diagonalizirati}, če obstaja nesingularna matrika $X$ in diagonalna matrika $\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n) $, 
da velja
\begin{align*}
    A = X\Lambda X^{-1} && \text{oz.} && AX = XA \\
\end{align*}
\begin{align*}
X = [\underbrace{x_1, \dots, x_n}_{\text{stolpci}}] && && Ax_i = X\begin{bmatrix}
    0 \\
    \vdots\\
    \lambda_i \\
    \vdots \\
    0
\end{bmatrix}
= \lambda_i x_i
\end{align*}

Matriki $A$ in $B$ sta podobni, če obstaja nesingularna matrika $S$, da velja $B = SAS^{-1}$.
Podobne matrike imajo iste lastne vrednosti.

Lastne vrednosti lahko (neučinkovito) izračunamo kot ničle \textbf{karakterističnega polinoma}
\[\det (A-\lambda I) = 0\] 

\subsection*{Schurova forma}
Za vsako matriko $A \in \mathbb{C}^{m\times n}$ obstaja unitarna matrika $U$ ($U^HU = I = UU^H$) 
in zgornje trikotna matrika $T$ (\textit{Schurova forma}), da velja
\[ A = UTU^H \]

Če pa je $A$ realna matrika, obstaja orotgonalna matrika $Q$ in kvazi (na diagonali dopuščamo $2\times 2$ bloke) 
zgornje trikotna matrika $R$, da velja
\[ A = QRQ^T \]

\subsection*{Potenčna metoda}
Za dano matriko $A$ iščemo \textbf{dominantno} (po absolutni vrednosti največjo) lastno vrednost
in pripadajoč lastni vektor.

Če imamo lastni vektor $x$, lahko pripadajočo lastno vrednost izrazimo z \textbf{Rayleighovim kvocientom}:
\[ \lambda = \frac{x^T A x}{\| x \|^2}\]

\begin{koda}
vhod: matrika $A$, zacetni prblizek $z_0$, $\| z_0 \|=1$, toleranca $\epsilon$

k = 0
ponavljaj:
    $y_{k+1} \leftarrow A z_k$
    $\rho_k \leftarrow z_k^T y_k$
    $z_{k+1} = \frac{y_{k+1}}{\| y_{k+1} \|}$
    $k \leftarrow k + 1$
dokler $(\| y_{k+1} - \rho_k z_k \| > \epsilon )$ in $(k \leq \text{max\_korakov})$
vrni $z_k, \rho_k$
\end{koda}

\end{multicols}
\end{document}